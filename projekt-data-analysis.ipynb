{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5346e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sam\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: spacy in c:\\users\\sam\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sam\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\sam\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sam\\anaconda3\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from gensim) (2.0.9)\n",
      "Requirement already satisfied: pyfume in c:\\users\\sam\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sam\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
      "Requirement already satisfied: simpful==2.12.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso==1.8.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\sam\\anaconda3\\lib\\site-packages (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/12.8 MB 656.4 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.2/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.8/12.8 MB 3.9 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 5.1 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.3/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.8/12.8 MB 7.8 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.0/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.9/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.8/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.6/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 13.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 13.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 14.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/12.8 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 21.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sam\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\sam\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#Installation der notwendigen Bibliotheken:\n",
    "!pip install pandas spacy scikit-learn gensim numpy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528f923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import der Bibliotheken & des spacy-Packets lt. Output\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ce57ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Stoppwörter:\n",
      "{'at', 'herein', 'itself', 'himself', 'twenty', 'below', 'ever', 'from', 'all', 'have', 'get', '’ve', 'he', 'behind', 'very', 'about', 'when', 'hers', 'same', 'toward', 'been', 'until', 'once', 'themselves', 'each', 'a', 'then', 'between', 'but', 'yourself', 'three', 'serious', 'part', 'eleven', 'yours', 'down', 'enough', 'against', 'since', 'beforehand', 'seeming', 'always', 'except', 'seemed', 'herself', 'no', 'anyhow', 'so', 'using', 'regarding', 'noone', 'is', 'hereupon', 'already', 'me', 'sixty', 'wherever', 'last', 'n’t', 'by', 'see', 'with', 'do', \"n't\", 'the', 'your', 'none', 'out', '‘d', 'latter', 'might', 'may', 'mine', 'am', 'ten', 'less', '’m', 'who', 'they', '’s', 'and', 'make', 'are', 'namely', 'top', 'was', 'whoever', 'name', 'again', 'amount', 'ca', 'him', 'five', 'perhaps', 'towards', 'across', 'still', 'us', 'such', 'became', 'has', 'four', 'any', 'third', 'elsewhere', 'indeed', 'only', 'beyond', 'front', 'or', 'could', 'say', 'seems', 'alone', 'whereas', 'really', 'above', \"'s\", 'never', \"'ll\", 'them', '‘re', 'while', 'meanwhile', \"'d\", 'his', 'either', 'becomes', 'few', 'please', 'yourselves', 'full', 'more', 'nine', 'besides', 'whatever', 'because', 'six', 'therein', 'her', 'nevertheless', 'along', 'over', 're', 'fifteen', 'those', 'beside', '’ll', 'being', 'else', 'become', 'someone', 'its', 'amongst', 'give', 'upon', 'would', 'well', 'otherwise', 'must', 'whether', 'even', 'everyone', 'bottom', 'will', 'wherein', 'nothing', 'however', 'she', 'per', 'n‘t', 'into', 'fifty', 'thence', 'that', 'under', 'doing', 'anyway', 'were', 'keep', 'every', 'further', 'than', 'before', 'these', 'how', 'sometime', 'nor', 'in', 'which', \"'m\", 'almost', 'just', 'thus', 'some', '’re', 'whenever', 'among', 'for', 'whole', 'throughout', 'around', 'quite', 'often', 'anyone', 'did', '‘m', 'thereafter', 'whereupon', 'former', 'afterwards', 'up', 'own', '’d', 'anywhere', 'does', 'ours', 'one', 'becoming', 'although', 'within', 'there', 'you', 'their', 'another', 'much', 'myself', 'two', 'off', 'thereby', 'what', 'as', '‘ll', 'it', 'first', 'here', 'though', 'others', 'move', 'something', 'thereupon', 'also', 'everywhere', 'both', 'this', 'yet', 'many', 'everything', 'put', 'whose', 'least', 'an', 'thru', \"'re\", 'to', 'hereby', 'forty', '‘s', 'anything', 'should', 'several', 'hundred', 'on', 'be', 'after', 'too', 'of', 'if', 'mostly', 'now', 'through', 'somewhere', 'moreover', 'unless', 'during', 'other', 'somehow', 'via', 'not', 'formerly', 'i', 'take', 'most', 'neither', 'call', 'whom', 'onto', 'whence', 'eight', 'side', 'due', 'back', 'can', 'without', 'hence', '‘ve', 'where', 'next', 'had', 'go', 'used', 'seem', 'sometimes', 'therefore', 'whereafter', 'whereby', 'cannot', 'rather', 'nobody', 'made', 'our', 'empty', 'hereafter', 'together', 'nowhere', 'twelve', 'latterly', 'ourselves', 'why', \"'ve\", 'show', 'we', 'whither', 'my', 'various', 'done'}\n"
     ]
    }
   ],
   "source": [
    "#Kontrolle und Anpassen der Stoppwörter:\n",
    "print(\"Original Stoppwörter:\")\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace059ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoppwörter:\n",
      "{'at', 'herein', 'itself', 'himself', 'twenty', 'below', 'ever', 'from', 'all', 'have', 'get', '’ve', 'he', 'behind', 'very', 'about', 'when', 'hers', 'same', 'toward', 'been', 'until', 'once', 'themselves', 'each', 'a', 'then', 'between', 'but', 'yourself', 'three', 'serious', 'part', 'eleven', 'yours', 'down', 'enough', 'against', 'since', 'beforehand', 'seeming', 'always', 'except', 'seemed', 'herself', 'anyhow', 'so', 'using', 'regarding', 'noone', 'is', 'hereupon', 'already', 'me', 'sixty', 'wherever', 'last', 'n’t', 'by', 'see', 'with', 'do', \"n't\", 'the', 'your', 'none', 'out', '‘d', 'latter', 'might', 'may', 'mine', 'am', 'ten', 'less', '’m', 'who', 'they', '’s', 'and', 'make', 'are', 'namely', 'top', 'was', 'whoever', 'name', 'again', 'amount', 'ca', 'him', 'five', 'perhaps', 'towards', 'across', 'still', 'us', 'such', 'became', 'has', 'four', 'any', 'third', 'elsewhere', 'indeed', 'only', 'beyond', 'front', 'or', 'could', 'say', 'seems', 'alone', 'whereas', 'really', 'above', \"'s\", 'never', \"'ll\", 'them', '‘re', 'while', 'meanwhile', \"'d\", 'his', 'either', 'becomes', 'few', 'please', 'yourselves', 'full', 'more', 'nine', 'besides', 'whatever', 'because', 'six', 'therein', 'her', 'nevertheless', 'along', 'over', 're', 'fifteen', 'those', 'beside', '’ll', 'being', 'else', 'become', 'someone', 'its', 'amongst', 'give', 'upon', 'would', 'well', 'otherwise', 'must', 'whether', 'even', 'everyone', 'bottom', 'will', 'wherein', 'nothing', 'however', 'she', 'per', 'n‘t', 'into', 'fifty', 'thence', 'that', 'under', 'doing', 'anyway', 'were', 'keep', 'every', 'further', 'than', 'before', 'these', 'how', 'sometime', 'nor', 'in', 'which', \"'m\", 'almost', 'just', 'thus', 'some', '’re', 'whenever', 'among', 'for', 'whole', 'throughout', 'around', 'quite', 'often', 'anyone', 'did', '‘m', 'thereafter', 'whereupon', 'former', 'afterwards', 'up', 'own', '’d', 'anywhere', 'does', 'ours', 'one', 'becoming', 'although', 'within', 'there', 'you', 'their', 'another', 'much', 'myself', 'two', 'off', 'thereby', 'what', 'as', '‘ll', 'it', 'first', 'here', 'though', 'others', 'move', 'something', 'thereupon', 'also', 'everywhere', 'both', 'this', 'yet', 'many', 'everything', 'put', 'whose', 'least', 'an', 'thru', \"'re\", 'to', 'hereby', 'forty', '‘s', 'anything', 'should', 'several', 'hundred', 'on', 'be', 'after', 'too', 'of', 'if', 'mostly', 'now', 'through', 'somewhere', 'moreover', 'unless', 'during', 'other', 'somehow', 'via', 'formerly', 'i', 'take', 'most', 'neither', 'call', 'whom', 'onto', 'whence', 'eight', 'side', 'due', 'back', 'can', 'without', 'hence', '‘ve', 'where', 'next', 'had', 'go', 'used', 'seem', 'sometimes', 'therefore', 'whereafter', 'whereby', 'cannot', 'rather', 'nobody', 'made', 'our', 'empty', 'hereafter', 'together', 'nowhere', 'twelve', 'latterly', 'ourselves', 'why', \"'ve\", 'show', 'we', 'whither', 'my', 'various', 'done'}\n"
     ]
    }
   ],
   "source": [
    "remove_stop_words = {'no', 'not'}\n",
    "nlp.Defaults.stop_words -= remove_stop_words\n",
    "\n",
    "print(\"Stoppwörter:\")\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32fce304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorverarbeitung des Textes:\n",
    "def preprocess_text(text):\n",
    "    #Textformat muss ein String sein:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #Sonderzeichen entfernen:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Umwandlung in Kleinbuchstaben: \n",
    "    doc = nlp(text.lower())\n",
    "    #Stoppwörter entfernen und Lemmatisierung:\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9abeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einlesen erfolgreich.\n"
     ]
    }
   ],
   "source": [
    "#CSV-Datei einlesen & ausführen der definierten Vorverarbeitung:\n",
    "csv_datei = 'amazon_reviews.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_datei)\n",
    "    print(\"Einlesen erfolgreich.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Fehler: Datei unter '{csv_datei}' nicht gefunden.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Fehler beim Parsen der CSV-Datei. Bitte überprüfen Sie das Dateiformat.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein unerwarteter Fehler ist aufgetreten: {e}\")\n",
    "    \n",
    "if 'df' in locals():\n",
    "    #Auswahl der Spalte 'reviewText'\n",
    "    if 'reviewText' in df.columns:\n",
    "        #Überprüfen und Bereinigen der Daten:\n",
    "        df['reviewText'] = df['reviewText'].astype(str)\n",
    "        df['cleaned_reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "    else:\n",
    "        print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29571623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der BoW-Methode mittels scikit-Learn:\n",
    "if 'cleaned_reviewText' in df.columns:\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(df['cleaned_reviewText'])\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ecbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der BoW-Methode mittels numpy:\n",
    "def bow_with_numpy(texts):\n",
    "    #Erstellen des Vokabulars\n",
    "    vocab = list(set(\" \".join(texts).split()))\n",
    "    vocab.sort()\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    #Erstellen der BoW-Matrix\n",
    "    bow_matrix = np.zeros((len(texts), len(vocab)))\n",
    "    for i, text in enumerate(texts):\n",
    "        word_count = Counter(text.split())\n",
    "        for word, count in word_count.items():\n",
    "            if word in vocab_dict:\n",
    "                bow_matrix[i, vocab_dict[word]] = count\n",
    "    return bow_matrix, vocab\n",
    "\n",
    "if 'cleaned_reviewText' in df.columns:\n",
    "    bow_matrix_np, vocab_np = bow_with_numpy(df['cleaned_reviewText'])\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a29cfc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-Vektoren (scikit-learn):\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "BoW-Vektoren (numpy):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Anzeigen der beiden erstellten Vektoren:\n",
    "#scikit-Learn:\n",
    "if 'X_bow' in locals():\n",
    "    print(\"BoW-Vektoren (scikit-learn):\")\n",
    "    print(X_bow.toarray())\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#numpy:\n",
    "if 'bow_matrix_np' in locals():\n",
    "    print(\"BoW-Vektoren (numpy):\")\n",
    "    print(bow_matrix_np)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8ea4c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topics:\n",
      "Topic 0:\n",
      "card gb phone sandisk work not sd memory no buy\n",
      "Topic 1:\n",
      "card class write mbs exfat test file mb result read\n",
      "Topic 2:\n",
      "phone card music restart problem play photo issue format try\n",
      "Topic 3:\n",
      "sandisk problem work card product issue gs star review replacement\n",
      "Topic 4:\n",
      "sandisk phone gb not product ultra class new speed problem\n",
      "LDA Topics:\n",
      "Topic 0:\n",
      "card gb format surface fat device exfat pro storage file\n",
      "Topic 1:\n",
      "work great no tablet galaxy buy card use price memory\n",
      "Topic 2:\n",
      "card speed video fast write gb camera gopro class no\n",
      "Topic 3:\n",
      "card phone gb work not memory galaxy samsung sandisk sd\n",
      "Topic 4:\n",
      "card work not sandisk good buy price sd product great\n"
     ]
    }
   ],
   "source": [
    "#Funktion zum Anzeigen der Themen:\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}:\")\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-top_n - 1:-1]]))\n",
    "\n",
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 5\n",
    "\n",
    "if 'X_bow' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(X_bow)\n",
    "\n",
    "    print(\"LSA Topics:\")\n",
    "    print_topics(lsa_model, vectorizer)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(X_bow)\n",
    "\n",
    "    print(\"LDA Topics:\")\n",
    "    print_topics(lda_model, vectorizer)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd24ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document assignments for LSA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note   read update scroll   m leave review   s...</td>\n",
       "      <td>hii order card arrive day fast prime unfortuna...</td>\n",
       "      <td>monday november   galaxy s format restart phon...</td>\n",
       "      <td>note   read update scroll   m leave review   s...</td>\n",
       "      <td>note   read update scroll   m leave review   s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monday november   galaxy s format restart phon...</td>\n",
       "      <td>test dozen sdhc microsdhc card disturb trend n...</td>\n",
       "      <td>buy use samsung galaxy s pop card mount format...</td>\n",
       "      <td>buy card originally galaxy note ii intention p...</td>\n",
       "      <td>update    lovely wife buy samsung galaxy tab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hii order card arrive day fast prime unfortuna...</td>\n",
       "      <td>buy card originally galaxy note ii intention p...</td>\n",
       "      <td>samsung galaxy note   come   gig internal memo...</td>\n",
       "      <td>buy   wife   android phone fail   month figure...</td>\n",
       "      <td>look class   microsd card order directly trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>update    lovely wife buy samsung galaxy tab...</td>\n",
       "      <td>purchase card sandisk packaging pretty sure ge...</td>\n",
       "      <td>update   suppose last forever year card go bad...</td>\n",
       "      <td>update    lovely wife buy samsung galaxy tab...</td>\n",
       "      <td>buy   wife   android phone fail   month figure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buy card originally galaxy note ii intention p...</td>\n",
       "      <td>buy gb version card use gopro hero   black edi...</td>\n",
       "      <td>phone recognize card follow suggestion format ...</td>\n",
       "      <td>problem sd card prior good track record sandis...</td>\n",
       "      <td>test dozen sdhc microsdhc card disturb trend n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Topic 0  \\\n",
       "0  note   read update scroll   m leave review   s...   \n",
       "1  monday november   galaxy s format restart phon...   \n",
       "2  hii order card arrive day fast prime unfortuna...   \n",
       "3    update    lovely wife buy samsung galaxy tab...   \n",
       "4  buy card originally galaxy note ii intention p...   \n",
       "\n",
       "                                             Topic 1  \\\n",
       "0  hii order card arrive day fast prime unfortuna...   \n",
       "1  test dozen sdhc microsdhc card disturb trend n...   \n",
       "2  buy card originally galaxy note ii intention p...   \n",
       "3  purchase card sandisk packaging pretty sure ge...   \n",
       "4  buy gb version card use gopro hero   black edi...   \n",
       "\n",
       "                                             Topic 2  \\\n",
       "0  monday november   galaxy s format restart phon...   \n",
       "1  buy use samsung galaxy s pop card mount format...   \n",
       "2  samsung galaxy note   come   gig internal memo...   \n",
       "3  update   suppose last forever year card go bad...   \n",
       "4  phone recognize card follow suggestion format ...   \n",
       "\n",
       "                                             Topic 3  \\\n",
       "0  note   read update scroll   m leave review   s...   \n",
       "1  buy card originally galaxy note ii intention p...   \n",
       "2  buy   wife   android phone fail   month figure...   \n",
       "3    update    lovely wife buy samsung galaxy tab...   \n",
       "4  problem sd card prior good track record sandis...   \n",
       "\n",
       "                                             Topic 4  \n",
       "0  note   read update scroll   m leave review   s...  \n",
       "1    update    lovely wife buy samsung galaxy tab...  \n",
       "2  look class   microsd card order directly trans...  \n",
       "3  buy   wife   android phone fail   month figure...  \n",
       "4  test dozen sdhc microsdhc card disturb trend n...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document assignments for LDA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>read place work android tablet find card requi...</td>\n",
       "      <td>es una reconocida marca y la calidad de sus pr...</td>\n",
       "      <td>purchase ultra microsdhc class   card medium p...</td>\n",
       "      <td>monday november   galaxy s format restart phon...</td>\n",
       "      <td>buy card fail usage   kicker favoris   month r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problem get format fat use myexcellent sansa c...</td>\n",
       "      <td>use small asus   notepad little extra storage ...</td>\n",
       "      <td>description amazon page call class   card uhs ...</td>\n",
       "      <td>buy card originally galaxy note ii intention p...</td>\n",
       "      <td>ve buy sandisk warranty reasoning lifetime war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>find write bandwidth card mbsec transfer stead...</td>\n",
       "      <td>micro disk fit samsung phone perfectly able us...</td>\n",
       "      <td>sandisk make lot claim card questionable highc...</td>\n",
       "      <td>update   suppose last forever year card go bad...</td>\n",
       "      <td>know issue sandisk gopro hero camera pay atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>old generation sandisk gb card bough   lot fas...</td>\n",
       "      <td>let thing bang pocket key coin accidentally co...</td>\n",
       "      <td>highly recommend sandisk ultra   gb microsdhc ...</td>\n",
       "      <td>nexus list support maximum gb sdcardbase   rep...</td>\n",
       "      <td>buy separately frustration free package appare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>little work card excellent way inexpensivly up...</td>\n",
       "      <td>use show time fast came wrap small paper packe...</td>\n",
       "      <td>description confusing   say time class   mean ...</td>\n",
       "      <td>order gb gb version memory card write review a...</td>\n",
       "      <td>past ve buy lot cf sd card usb key year ago bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Topic 0  \\\n",
       "0  read place work android tablet find card requi...   \n",
       "1  problem get format fat use myexcellent sansa c...   \n",
       "2  find write bandwidth card mbsec transfer stead...   \n",
       "3  old generation sandisk gb card bough   lot fas...   \n",
       "4  little work card excellent way inexpensivly up...   \n",
       "\n",
       "                                             Topic 1  \\\n",
       "0  es una reconocida marca y la calidad de sus pr...   \n",
       "1  use small asus   notepad little extra storage ...   \n",
       "2  micro disk fit samsung phone perfectly able us...   \n",
       "3  let thing bang pocket key coin accidentally co...   \n",
       "4  use show time fast came wrap small paper packe...   \n",
       "\n",
       "                                             Topic 2  \\\n",
       "0  purchase ultra microsdhc class   card medium p...   \n",
       "1  description amazon page call class   card uhs ...   \n",
       "2  sandisk make lot claim card questionable highc...   \n",
       "3  highly recommend sandisk ultra   gb microsdhc ...   \n",
       "4  description confusing   say time class   mean ...   \n",
       "\n",
       "                                             Topic 3  \\\n",
       "0  monday november   galaxy s format restart phon...   \n",
       "1  buy card originally galaxy note ii intention p...   \n",
       "2  update   suppose last forever year card go bad...   \n",
       "3  nexus list support maximum gb sdcardbase   rep...   \n",
       "4  order gb gb version memory card write review a...   \n",
       "\n",
       "                                             Topic 4  \n",
       "0  buy card fail usage   kicker favoris   month r...  \n",
       "1  ve buy sandisk warranty reasoning lifetime war...  \n",
       "2  know issue sandisk gopro hero camera pay atten...  \n",
       "3  buy separately frustration free package appare...  \n",
       "4  past ve buy lot cf sd card usb key year ago bu...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 1: card, gb, phone, sandisk, work, not, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 2: card, class, write, mbs, exfat, test,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic 3: phone, card, music, restart, problem,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic 4: sandisk, problem, work, card, product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic 5: sandisk, phone, gb, not, product, ult...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Top Words\n",
       "0  Topic 1: card, gb, phone, sandisk, work, not, ...\n",
       "1  Topic 2: card, class, write, mbs, exfat, test,...\n",
       "2  Topic 3: phone, card, music, restart, problem,...\n",
       "3  Topic 4: sandisk, problem, work, card, product...\n",
       "4  Topic 5: sandisk, phone, gb, not, product, ult..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 1: card, gb, format, surface, fat, devic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 2: work, great, no, tablet, galaxy, buy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic 3: card, speed, video, fast, write, gb, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic 4: card, phone, gb, work, not, memory, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic 5: card, work, not, sandisk, good, buy, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Top Words\n",
       "0  Topic 1: card, gb, format, surface, fat, devic...\n",
       "1  Topic 2: work, great, no, tablet, galaxy, buy,...\n",
       "2  Topic 3: card, speed, video, fast, write, gb, ...\n",
       "3  Topic 4: card, phone, gb, work, not, memory, g...\n",
       "4  Topic 5: card, work, not, sandisk, good, buy, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Funktion zum Zuweisen der Themen zu den Dokumenten:\n",
    "def assign_topics(topic_matrix, n_top_documents=5):\n",
    "    assignments = {}\n",
    "    for topic_idx, topic in enumerate(topic_matrix.T):\n",
    "        assignments[f'Topic {topic_idx}'] = []\n",
    "        top_document_indices = topic.argsort()[:-n_top_documents - 1:-1]\n",
    "        for doc_index in top_document_indices:\n",
    "            assignments[f'Topic {topic_idx}'].append(df['cleaned_reviewText'].iloc[doc_index])\n",
    "    return assignments\n",
    "\n",
    "#Funktion zum Erstellen eines DataFrames:\n",
    "def create_topic_dataframe(model, vectorizer, n_top_words=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(f\"Topic {topic_idx + 1}: \" + \", \".join(topic_words))\n",
    "    return pd.DataFrame(topics, columns=[\"Top Words\"])\n",
    "\n",
    "if 'lsa_topic_matrix' in locals() and 'lda_topic_matrix' in locals():\n",
    "    #Dokumentzuweisungen für LSA:\n",
    "    print(\"Document assignments for LSA:\")\n",
    "    lsa_assignments = assign_topics(lsa_topic_matrix)\n",
    "    lsa_assignments_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in lsa_assignments.items()]))\n",
    "    display(lsa_assignments_df)\n",
    "\n",
    "    #Dokumentzuweisungen für LDA:\n",
    "    print(\"Document assignments for LDA:\")\n",
    "    lda_assignments = assign_topics(lda_topic_matrix)\n",
    "    lda_assignments_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in lda_assignments.items()]))\n",
    "    display(lda_assignments_df)\n",
    "\n",
    "    #DataFrames für die Themen:\n",
    "    lsa_topics_df = create_topic_dataframe(lsa_model, vectorizer)\n",
    "    lda_topics_df = create_topic_dataframe(lda_model, vectorizer)\n",
    "\n",
    "    print(\"LSA Topics:\")\n",
    "    display(lsa_topics_df)\n",
    "\n",
    "    print(\"LDA Topics:\")\n",
    "    display(lda_topics_df)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e83681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
