{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36749232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import der Bibliotheken & des spacy-Packets\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA, TruncatedSVD\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d00b631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Stoppwörter:\n",
      "{'two', 'empty', 'eleven', 'done', 'often', 'seems', 'did', 'up', 'upon', 'serious', 'forty', 'thereby', 'always', 'behind', 'whereafter', 'on', 'than', 'themselves', 'via', 'less', 'were', 'least', 'whom', 'whereby', 'over', 'noone', \"'d\", 'various', 'part', 'even', 'take', 'per', 'at', 'anything', 'becomes', 'where', 'own', 'last', 'either', 'since', 'whereas', 'your', 'both', 'there', 'indeed', 'does', 'of', '‘re', 'front', 'same', 'four', 'twenty', 'would', 'after', 'been', 'go', 'should', 'nevertheless', 'no', 'full', 'hereupon', 'those', 'whatever', 'between', 'none', 'anyway', 'thereupon', 'beforehand', 'whether', 'others', 'us', 'an', 'to', 'nine', 'beyond', 'it', 'regarding', \"n't\", 'whoever', 'every', 'mine', 'within', 'onto', 'whereupon', 'well', 'nor', 'its', 'everyone', 'what', 'meanwhile', '’ll', 'very', 'as', '’re', 'amount', 'ours', 'may', 'why', 'top', 'the', 'however', 'will', \"'re\", 'him', 'used', 'ourselves', 'further', 'also', 'made', 'across', 'herein', 'afterwards', 'whose', 'each', 'for', 'many', 'down', 'that', 'yourself', 'towards', 'become', \"'m\", 'everything', 'just', 'several', 'has', 'thence', 'seemed', 'have', 'became', 'unless', 'under', 'all', 'being', 'whole', 'yours', 'a', 'quite', 'such', 'besides', 'together', 'wherein', 'if', 'into', 'without', 'yet', 'you', 'five', 'neither', 'side', 'beside', 'put', 'above', 'herself', 'they', 'too', 'much', 'might', 'get', 'these', 'wherever', 'cannot', 'more', 'how', 'yourselves', 'nowhere', 'most', 'any', 're', 'this', '‘ve', 'name', 'is', 'our', 'he', 'keep', 'during', 'itself', 'nobody', 'with', 'hundred', 'ever', 'thru', 'while', 'elsewhere', 'hers', 'we', 'otherwise', 'former', 'another', 'was', 'about', 'are', 'due', 'six', 'doing', 'alone', 'when', 'move', 'still', 'whence', 'do', 'other', 'and', \"'ll\", 'almost', 'next', 'seeming', 'somehow', 'must', 'can', \"'ve\", \"'s\", 'be', '‘d', 'call', 'whither', 'them', 'moreover', 'along', 'n’t', 'n‘t', 'somewhere', 'mostly', 'ten', 'back', 'now', 'else', 'only', 'from', 'anyone', 'again', 'ca', 'once', 'twelve', '’d', 'because', '’ve', 'throughout', 'third', 'sometime', 'not', 'had', 'formerly', 'in', 'make', 'someone', 'out', 'amongst', 'off', 'she', 'hereafter', 'please', 'give', 'who', 'perhaps', 'latterly', 'which', 'am', 'by', 'already', 'seem', 'though', 'really', 'until', 'so', 'sometimes', 'everywhere', 'around', 'sixty', 'their', 'say', 'her', 'show', 'namely', 'hence', 'through', 'here', 'himself', 'therein', 'whenever', 'rather', 'nothing', 'before', 'something', 'myself', 'therefore', 'using', 'below', 'my', 'me', 'eight', 'then', 'except', 'i', 'see', 'against', 'thereafter', 'some', 'first', '‘m', 'although', 'three', 'fifty', '’s', 'toward', 'bottom', 'his', 'thus', 'one', 'hereby', 'but', 'latter', 'becoming', 'among', 'enough', '‘s', 'few', 'fifteen', '’m', 'anyhow', 'anywhere', 'could', '‘ll', 'or', 'never'}\n"
     ]
    }
   ],
   "source": [
    "#Kontrolle und Anpassen der Stoppwörter:\n",
    "print(\"Original Stoppwörter:\")\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9767e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoppwörter:\n",
      "{'two', 'empty', 'eleven', 'done', 'often', 'seems', 'did', 'up', 'upon', 'serious', 'forty', 'thereby', 'always', 'behind', 'whereafter', 'on', 'than', 'themselves', 'via', 'less', 'were', 'least', 'whom', 'whereby', 'over', 'noone', \"'d\", 'various', 'part', 'even', 'take', 'per', 'at', 'anything', 'becomes', 'where', 'own', 'last', 'either', 'since', 'whereas', 'your', 'both', 'there', 'indeed', 'does', 'of', '‘re', 'front', 'same', 'four', 'twenty', 'would', 'after', 'been', 'go', 'should', 'nevertheless', 'full', 'hereupon', 'those', 'whatever', 'between', 'none', 'anyway', 'thereupon', 'beforehand', 'whether', 'others', 'us', 'an', 'to', 'nine', 'beyond', 'it', 'regarding', \"n't\", 'whoever', 'every', 'mine', 'within', 'onto', 'whereupon', 'well', 'nor', 'its', 'everyone', 'what', 'meanwhile', '’ll', 'very', 'as', '’re', 'amount', 'ours', 'may', 'why', 'top', 'the', 'however', 'will', \"'re\", 'him', 'used', 'ourselves', 'further', 'also', 'made', 'sandisk', 'across', 'herein', 'afterwards', 'whose', 'each', 'for', 'many', 'down', 'that', 'yourself', 'towards', 'become', \"'m\", 'everything', 'just', 'several', 'has', 'thence', 'seemed', 'have', 'became', 'unless', 'under', 'all', 'being', 'whole', 'yours', 'a', 'quite', 'such', 'besides', 'together', 'wherein', 'if', 'into', 'without', 'yet', 'you', 'five', 'neither', 'side', 'beside', 'put', 'above', 'herself', 'they', 'too', 'much', 'might', 'get', 'these', 'wherever', 'cannot', 'more', 'how', 'yourselves', 'nowhere', 'most', 'any', 're', 'this', '‘ve', 'name', 'is', 'our', 'he', 'keep', 'during', 'itself', 'nobody', 'with', 'hundred', 'ever', 'thru', 'while', 'elsewhere', 'hers', 'we', 'otherwise', 'former', 'another', 'was', 'about', 'are', 'due', 'six', 'doing', 'alone', 'when', 'move', 'still', 'whence', 'do', 'other', 'and', \"'ll\", 'almost', 'next', 'card', 'seeming', 'somehow', 'must', 'can', \"'ve\", \"'s\", 'be', '‘d', 'call', 'whither', 'them', 'moreover', 'along', 'n’t', 'n‘t', 'somewhere', 'mostly', 'ten', 'back', 'now', 'else', 'only', 'from', 'anyone', 'again', 'ca', 'once', 'twelve', '’d', 'because', '’ve', 'throughout', 'third', 'sometime', 'had', 'formerly', 'in', 'make', 'someone', 'out', 'amongst', 'off', 'she', 'hereafter', 'please', 'give', 'who', 'perhaps', 'latterly', 'which', 'am', 'by', 'already', 'seem', 'though', 'really', 'until', 'so', 'sometimes', 'everywhere', 'around', 'sixty', 'their', 'say', 'her', 'show', 'namely', 'hence', 'through', 'here', 'himself', 'therein', 'whenever', 'rather', 'phone', 'nothing', 'before', 'something', 'myself', 'therefore', 'using', 'below', 'my', 'me', 'eight', 'then', 'except', 'i', 'see', 'against', 'thereafter', 'some', 'first', '‘m', 'although', 'three', 'fifty', '’s', 'toward', 'bottom', 'his', 'thus', 'one', 'hereby', 'but', 'latter', 'becoming', 'among', 'enough', '‘s', 'few', 'fifteen', '’m', 'anyhow', 'anywhere', 'could', '‘ll', 'or', 'never'}\n"
     ]
    }
   ],
   "source": [
    "additional_stop_words = {'card', 'sandisk', 'phone'}\n",
    "nlp.Defaults.stop_words |= additional_stop_words\n",
    "\n",
    "remove_stop_words = {'no', 'not'}\n",
    "nlp.Defaults.stop_words -= remove_stop_words\n",
    "\n",
    "print(\"Stoppwörter:\")\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6425aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorverarbeitung des Textes:\n",
    "def preprocess_text(text):\n",
    "    #Textformat muss ein String sein:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #Sonderzeichen entfernen:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Umwandlung in Kleinbuchstaben: \n",
    "    doc = nlp(text.lower())\n",
    "    #Stoppwörter entfernen und Lemmatisierung:\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877fd4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einlesen erfolgreich.\n"
     ]
    }
   ],
   "source": [
    "#CSV-Datei einlesen & ausführen der definierten Vorverarbeitung:\n",
    "csv_datei = 'amazon_reviews.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_datei)\n",
    "    print(\"Einlesen erfolgreich.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Fehler: Datei unter '{csv_datei}' nicht gefunden.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Fehler beim Parsen der CSV-Datei. Bitte überprüfen Sie das Dateiformat.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein unerwarteter Fehler ist aufgetreten: {e}\")\n",
    "    \n",
    "if 'df' in locals():\n",
    "    #Auswahl der Spalte 'reviewText'\n",
    "    if 'reviewText' in df.columns:\n",
    "        #Überprüfen und Bereinigen der Daten:\n",
    "        df['reviewText'] = df['reviewText'].astype(str)\n",
    "        df['cleaned_reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "    else:\n",
    "        print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77dae641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der BoW-Methode mittels scikit-Learn:\n",
    "if 'cleaned_reviewText' in df.columns:\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(df['cleaned_reviewText'])\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455091ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der BoW-Methode mittels numpy:\n",
    "def bow_with_numpy(texts):\n",
    "    #Erstellen des Vokabulars\n",
    "    vocab = list(set(\" \".join(texts).split()))\n",
    "    vocab.sort()\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    #Erstellen der BoW-Matrix\n",
    "    bow_matrix = np.zeros((len(texts), len(vocab)))\n",
    "    for i, text in enumerate(texts):\n",
    "        word_count = Counter(text.split())\n",
    "        for word, count in word_count.items():\n",
    "            if word in vocab_dict:\n",
    "                bow_matrix[i, vocab_dict[word]] = count\n",
    "    return bow_matrix, vocab\n",
    "\n",
    "if 'cleaned_reviewText' in df.columns:\n",
    "    bow_matrix_np, vocab_np = bow_with_numpy(df['cleaned_reviewText'])\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767b5a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-Vektoren (scikit-learn):\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "BoW-Vektoren (numpy):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Anzeigen der beiden erstellten Vektoren:\n",
    "#scikit-Learn:\n",
    "if 'X_bow' in locals():\n",
    "    print(\"BoW-Vektoren (scikit-learn):\")\n",
    "    print(X_bow.toarray())\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#numpy:\n",
    "if 'bow_matrix_np' in locals():\n",
    "    print(\"BoW-Vektoren (numpy):\")\n",
    "    print(bow_matrix_np)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac9a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der Tf-idf-Methode:\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093ff8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topics:\n",
      "Topic 0:\n",
      "videoi mobiledelivery card wav photographic defect vivotab immense mistake maxx\n",
      "Topic 1:\n",
      "job photographic mobiledelivery plier intentionally card immense spend rez\n",
      "Topic 2:\n",
      "mobiledelivery st wav formatea dd fore policythe unneeded\n",
      "Topic 3:\n",
      "mistake itndeliver retch job splurge st rightyou\n",
      "Topic 4:\n",
      "plier kit mo rez hide laptoptransfer thetranscend mobiledelivery cardonce\n",
      "LDA Topics:\n",
      "Topic 0:\n",
      "limitenjoy readily minicamcorder computation sandiskfrom asus succede outgrow computerdont\n",
      "Topic 1:\n",
      "mobiledelivery videoi card wav photographic vivotab defect immense maxx prepard\n",
      "Topic 2:\n",
      "kitten consumer company foray predictably mbsthe issuesnot heyfer benchmarksetting insert\n",
      "Topic 3:\n",
      "videoi mobiledelivery card mistake immense splurge defect vivotab photographic\n",
      "Topic 4:\n",
      "gti bend expound gate ui experiment daughter cect waswhile\n"
     ]
    }
   ],
   "source": [
    "#Funktion zum Anzeigen der Themen:\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}:\")\n",
    "        topic_words = [words[i] for i in topic.argsort()[:-top_n - 1:-1] if i < len(words)]\n",
    "        print(\" \".join(topic_words)) \n",
    "\n",
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 5\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    print(\"LSA Topics:\")\n",
    "    print_topics(lsa_model, vectorizer)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    print(\"LDA Topics:\")\n",
    "    print_topics(lda_model, vectorizer)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89724ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumentenzuweisungen für LSA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note   read update scroll   m leave review   s...</td>\n",
       "      <td>work great</td>\n",
       "      <td>work perfect no complaint fast s buy say go</td>\n",
       "      <td>memory work   price good size</td>\n",
       "      <td>no issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>space droid razr gb microsd slot figure d pick...</td>\n",
       "      <td>work great galaxy s</td>\n",
       "      <td>work like work like buy thinking work</td>\n",
       "      <td>capacity price not wrong   m sure class recomm...</td>\n",
       "      <td>no issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buy originally galaxy note ii intention primar...</td>\n",
       "      <td>galaxy s   work great ve no problem highly rec...</td>\n",
       "      <td>buy samsung galaxy siii work fine suppose</td>\n",
       "      <td>work   no problem   good price brand sd   buy</td>\n",
       "      <td>no problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>monday november   galaxy s format restart show...</td>\n",
       "      <td>use samsung galaxy   cell work great   good va...</td>\n",
       "      <td>disk work happy</td>\n",
       "      <td>great price work great   want storage   reason...</td>\n",
       "      <td>buy multiple problem recognize far no problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>update    lovely wife buy samsung galaxy tab...</td>\n",
       "      <td>use gopro hero   camera work great no issue pe...</td>\n",
       "      <td>hard believe memory store small chip</td>\n",
       "      <td>work great</td>\n",
       "      <td>month no problem awesome thank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Topic 0  \\\n",
       "0  note   read update scroll   m leave review   s...   \n",
       "1  space droid razr gb microsd slot figure d pick...   \n",
       "2  buy originally galaxy note ii intention primar...   \n",
       "3  monday november   galaxy s format restart show...   \n",
       "4    update    lovely wife buy samsung galaxy tab...   \n",
       "\n",
       "                                             Topic 1  \\\n",
       "0                                         work great   \n",
       "1                                work great galaxy s   \n",
       "2  galaxy s   work great ve no problem highly rec...   \n",
       "3  use samsung galaxy   cell work great   good va...   \n",
       "4  use gopro hero   camera work great no issue pe...   \n",
       "\n",
       "                                       Topic 2  \\\n",
       "0  work perfect no complaint fast s buy say go   \n",
       "1        work like work like buy thinking work   \n",
       "2    buy samsung galaxy siii work fine suppose   \n",
       "3                              disk work happy   \n",
       "4         hard believe memory store small chip   \n",
       "\n",
       "                                             Topic 3  \\\n",
       "0                      memory work   price good size   \n",
       "1  capacity price not wrong   m sure class recomm...   \n",
       "2      work   no problem   good price brand sd   buy   \n",
       "3  great price work great   want storage   reason...   \n",
       "4                                         work great   \n",
       "\n",
       "                                           Topic 4  \n",
       "0                                         no issue  \n",
       "1                                         no issue  \n",
       "2                                       no problem  \n",
       "3    buy multiple problem recognize far no problem  \n",
       "4                   month no problem awesome thank  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumentenzuweisungen für LDA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>este fue una buena compras ha sido todo lo que...</td>\n",
       "      <td>note   read update scroll   m leave review   s...</td>\n",
       "      <td>comore esta memorka para completar mi nueva sa...</td>\n",
       "      <td>video recording device surveillance carbase dv...</td>\n",
       "      <td>es un producto que recomiendo ampliamente solo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es una tarjeta micro guarda todas la caracteri...</td>\n",
       "      <td>get wife samsung galaxy tab    christmas take ...</td>\n",
       "      <td>es una reconocida marca y la calidad de sus pr...</td>\n",
       "      <td>not reliable handle hd video gopro hero   glit...</td>\n",
       "      <td>la uso sul mio samsung galaxy tab    e va beni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>muy bueno realmente es rapido como dice mi gal...</td>\n",
       "      <td>buy originally galaxy note ii intention primar...</td>\n",
       "      <td>fast giv work great</td>\n",
       "      <td>ve try card go compact flash card rule single ...</td>\n",
       "      <td>oh gosh great invention slice bread total unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product expectedcrystaldiskmark   x c   hiyohi...</td>\n",
       "      <td>primary reason buy extra capacitysandisk ultra...</td>\n",
       "      <td>bouthg memory contry store sele good product b...</td>\n",
       "      <td>mobius action camera p hd mini sport camdash c...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>announcement gb micro sd take internet storm p...</td>\n",
       "      <td>buy memory replace gb memory zte valey phonean...</td>\n",
       "      <td>format sdformatter pc test sony vaio cd sz   v...</td>\n",
       "      <td>excelent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Topic 0  \\\n",
       "0  este fue una buena compras ha sido todo lo que...   \n",
       "1  es una tarjeta micro guarda todas la caracteri...   \n",
       "2  muy bueno realmente es rapido como dice mi gal...   \n",
       "3  product expectedcrystaldiskmark   x c   hiyohi...   \n",
       "4                                                yes   \n",
       "\n",
       "                                             Topic 1  \\\n",
       "0  note   read update scroll   m leave review   s...   \n",
       "1  get wife samsung galaxy tab    christmas take ...   \n",
       "2  buy originally galaxy note ii intention primar...   \n",
       "3  primary reason buy extra capacitysandisk ultra...   \n",
       "4  announcement gb micro sd take internet storm p...   \n",
       "\n",
       "                                             Topic 2  \\\n",
       "0  comore esta memorka para completar mi nueva sa...   \n",
       "1  es una reconocida marca y la calidad de sus pr...   \n",
       "2                                fast giv work great   \n",
       "3  bouthg memory contry store sele good product b...   \n",
       "4  buy memory replace gb memory zte valey phonean...   \n",
       "\n",
       "                                             Topic 3  \\\n",
       "0  video recording device surveillance carbase dv...   \n",
       "1  not reliable handle hd video gopro hero   glit...   \n",
       "2  ve try card go compact flash card rule single ...   \n",
       "3  mobius action camera p hd mini sport camdash c...   \n",
       "4  format sdformatter pc test sony vaio cd sz   v...   \n",
       "\n",
       "                                             Topic 4  \n",
       "0  es un producto que recomiendo ampliamente solo...  \n",
       "1  la uso sul mio samsung galaxy tab    e va beni...  \n",
       "2  oh gosh great invention slice bread total unde...  \n",
       "3                                                nan  \n",
       "4                                           excelent  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 1: videoi, mobiledelivery, card, wav, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 2: job, photographic, mobiledelivery, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic 3: mobiledelivery, st, wav, formatea, dd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic 4: mistake, itndeliver, retch, job, splu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic 5: plier, kit, mo, rez, hide, laptoptran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Top Words\n",
       "0  Topic 1: videoi, mobiledelivery, card, wav, ph...\n",
       "1  Topic 2: job, photographic, mobiledelivery, pl...\n",
       "2  Topic 3: mobiledelivery, st, wav, formatea, dd...\n",
       "3  Topic 4: mistake, itndeliver, retch, job, splu...\n",
       "4  Topic 5: plier, kit, mo, rez, hide, laptoptran..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 1: limitenjoy, readily, minicamcorder, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 2: mobiledelivery, videoi, card, wav, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic 3: kitten, consumer, company, foray, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic 4: videoi, mobiledelivery, card, mistake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic 5: gti, bend, expound, gate, ui, experim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Top Words\n",
       "0  Topic 1: limitenjoy, readily, minicamcorder, c...\n",
       "1  Topic 2: mobiledelivery, videoi, card, wav, ph...\n",
       "2  Topic 3: kitten, consumer, company, foray, pre...\n",
       "3  Topic 4: videoi, mobiledelivery, card, mistake...\n",
       "4  Topic 5: gti, bend, expound, gate, ui, experim..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Funktion zum Zuweisen der Themen zu den Dokumenten:\n",
    "def assign_topics(topic_matrix, n_top_documents=5):\n",
    "    assignments = {}\n",
    "    for topic_idx, topic in enumerate(topic_matrix.T):\n",
    "        assignments[f'Topic {topic_idx}'] = []\n",
    "        top_document_indices = topic.argsort()[:-n_top_documents - 1:-1]\n",
    "        for doc_index in top_document_indices:\n",
    "            assignments[f'Topic {topic_idx}'].append(df['cleaned_reviewText'].iloc[doc_index])\n",
    "    return assignments\n",
    "\n",
    "#Funktion zum Erstellen eines DataFrames:\n",
    "def create_topic_dataframe(model, vectorizer, n_top_words=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1] if i < len(words)]\n",
    "        topics.append(f\"Topic {topic_idx + 1}: \" + \", \".join(topic_words))\n",
    "    return pd.DataFrame(topics, columns=[\"Top Words\"])\n",
    "\n",
    "if 'lsa_topic_matrix' in locals() and 'lda_topic_matrix' in locals():\n",
    "    #Dokumentzuweisungen für LSA:\n",
    "    print(\"Dokumentenzuweisungen für LSA:\")\n",
    "    lsa_assignments = assign_topics(lsa_topic_matrix)\n",
    "    lsa_assignments_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in lsa_assignments.items()]))\n",
    "    display(lsa_assignments_df)\n",
    "\n",
    "    #Dokumentzuweisungen für LDA:\n",
    "    print(\"Dokumentenzuweisungen für LDA:\")\n",
    "    lda_assignments = assign_topics(lda_topic_matrix)\n",
    "    lda_assignments_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in lda_assignments.items()]))\n",
    "    display(lda_assignments_df)\n",
    "\n",
    "    #DataFrames für die Themen:\n",
    "    lsa_topics_df = create_topic_dataframe(lsa_model, vectorizer)\n",
    "    lda_topics_df = create_topic_dataframe(lda_model, vectorizer)\n",
    "\n",
    "    print(\"LSA Topics:\")\n",
    "    display(lsa_topics_df)\n",
    "\n",
    "    print(\"LDA Topics:\")\n",
    "    display(lda_topics_df)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd58dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
