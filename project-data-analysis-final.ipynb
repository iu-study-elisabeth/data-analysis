{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca608f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import der Bibliotheken & des spacy-Packets\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA, TruncatedSVD\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f5efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Stoppwörter:\n",
      "{'back', 'name', 'such', 'over', 'during', 'against', 'become', \"'re\", 'otherwise', 'none', 'also', 'until', '‘ll', 'below', 'was', 'indeed', 'hereafter', 'go', 'two', 'empty', 'them', 'however', 'thereupon', 'are', 'mostly', 'beforehand', 'just', 'others', 'per', 'am', 'him', 'not', 'enough', 'never', 'latter', 'serious', 'few', 'ours', 'between', 'have', 'five', 'seem', 'must', 'her', 'whither', 'yourselves', 'nobody', '’m', 'see', 'than', 'becomes', 'out', 'somehow', 'cannot', 'thru', 'hers', 'can', 'beyond', 'these', 'on', 'himself', 'say', 'might', 'well', 'its', 'where', 'which', 'herein', 'former', 'if', 'nevertheless', 'did', '’ll', '’ve', 'to', 'along', 'whereafter', 'something', 'seemed', 'whereas', 'the', 'is', 'through', 'everyone', 'he', 'toward', 'no', 'you', 'rather', 'behind', 'whence', 'above', 'we', 'same', 'do', 'who', 'each', 'wherever', 'nine', 'in', 'hereby', 'yet', 'hundred', 'further', 'either', 'else', 'does', 'front', 'wherein', 'or', 'together', 'noone', 'anyone', 'regarding', 'own', '‘ve', 'onto', 'becoming', 'thence', 'another', 'amount', 'whose', '‘m', 'be', 'nor', 'show', \"'s\", 'sixty', 'as', 'some', 'myself', 'more', 'a', 'became', 'me', 'besides', 'his', 'except', 'whenever', 'too', \"'ll\", 'used', 'will', 'put', 'it', 'due', 'call', 'at', 'n‘t', 'whole', 'neither', '’d', 'up', 'of', 'should', 'may', 'she', 'side', 'while', 'anything', 'itself', 'many', 'forty', 'mine', 'ourselves', 'other', 'elsewhere', 'before', 'last', 'four', 'perhaps', 'always', 'since', 'throughout', 'across', 'after', 'whom', 'for', 'towards', 'down', 'often', 'make', 'done', 'us', 'around', 'then', 'really', 'move', 'nowhere', 'meanwhile', 'get', 'whoever', 'therefore', 'i', 'without', 'various', 'would', '’s', 'whereby', 'already', 'third', 'when', 'namely', 'themselves', 'alone', 'bottom', 'with', 'from', 'yourself', 'therein', 'that', 'has', 'unless', 'were', 'beside', 'whether', 'ever', 'an', 'now', 'one', 'several', 'whatever', 'under', 'very', 'made', 'thus', 'almost', 'somewhere', \"n't\", '‘d', 'n’t', 'latterly', 'everything', 'please', 'full', '‘s', 'much', 'six', 'among', 'even', 'sometimes', 'nothing', 'ca', 'take', 'so', 'this', \"'d\", 'by', 'here', 'part', 'amongst', 'how', 'someone', 'hence', 'thereby', 'about', 'anywhere', 'again', 'they', '’re', 'most', 'fifty', 'within', 'give', 'any', 'only', 'twenty', 'using', 'all', 'least', 'because', 'everywhere', 'sometime', 'twelve', 'anyhow', 'had', 'moreover', 'both', 'their', 'via', 'anyway', 'eleven', 'and', 're', 'thereafter', 'herself', 'off', 'next', 'although', 'three', 'into', 'quite', 'our', 'what', 'being', 'there', 'fifteen', 'ten', \"'ve\", 'whereupon', 'top', 'seeming', 'but', 'hereupon', 'seems', 'yours', 'though', 'upon', 'doing', 'my', 'why', 'been', 'still', 'afterwards', 'formerly', \"'m\", 'first', 'eight', 'once', '‘re', 'could', 'keep', 'less', 'every', 'those', 'your'}\n"
     ]
    }
   ],
   "source": [
    "#Kontrolle und Anpassen der Stoppwörter:\n",
    "print(\"Original Stoppwörter:\")\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d7b168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoppwörter:\n",
      "{'back', 'name', 'such', 'over', 'during', 'against', 'become', \"'re\", 'otherwise', 'none', 'also', 'until', '‘ll', 'below', 'was', 'indeed', 'hereafter', 'go', 'two', 'empty', 'them', 'however', 'thereupon', 'are', 'mostly', 'beforehand', 'just', 'others', 'per', 'am', 'him', 'enough', 'never', 'latter', 'serious', 'few', 'ours', 'between', 'have', 'five', 'seem', 'must', 'her', 'whither', 'yourselves', 'nobody', '’m', 'see', 'than', 'becomes', 'out', 'somehow', 'cannot', 'thru', 'hers', 'can', 'beyond', 'these', 'on', 'himself', 'say', 'might', 'well', 'its', 'where', 'which', 'herein', 'former', 'if', 'nevertheless', 'did', '’ll', '’ve', 'to', 'along', 'whereafter', 'something', 'seemed', 'whereas', 'the', 'is', 'through', 'everyone', 'he', 'toward', 'you', 'rather', 'behind', 'whence', 'above', 'we', 'same', 'do', 'who', 'each', 'wherever', 'nine', 'in', 'hereby', 'yet', 'hundred', 'further', 'either', 'else', 'does', 'front', 'wherein', 'or', 'together', 'noone', 'anyone', 'regarding', 'own', '‘ve', 'onto', 'becoming', 'thence', 'another', 'amount', 'whose', '‘m', 'be', 'nor', 'show', \"'s\", 'sixty', 'as', 'some', 'myself', 'more', 'a', 'became', 'me', 'besides', 'his', 'except', 'whenever', 'too', \"'ll\", 'used', 'will', 'put', 'it', 'due', 'call', 'at', 'n‘t', 'whole', 'neither', '’d', 'up', 'of', 'should', 'may', 'she', 'side', 'while', 'anything', 'itself', 'many', 'forty', 'mine', 'ourselves', 'other', 'elsewhere', 'before', 'last', 'four', 'perhaps', 'always', 'since', 'throughout', 'across', 'after', 'whom', 'for', 'towards', 'down', 'often', 'make', 'done', 'us', 'around', 'then', 'really', 'move', 'nowhere', 'meanwhile', 'get', 'whoever', 'therefore', 'i', 'without', 'various', 'would', '’s', 'whereby', 'already', 'third', 'when', 'namely', 'themselves', 'alone', 'phone', 'bottom', 'with', 'from', 'yourself', 'therein', 'that', 'has', 'unless', 'were', 'beside', 'whether', 'card', 'ever', 'an', 'now', 'one', 'several', 'whatever', 'under', 'very', 'made', 'thus', 'almost', 'somewhere', \"n't\", '‘d', 'n’t', 'latterly', 'everything', 'please', 'full', '‘s', 'much', 'six', 'among', 'even', 'sometimes', 'nothing', 'ca', 'take', 'so', 'this', \"'d\", 'by', 'here', 'part', 'amongst', 'how', 'someone', 'hence', 'thereby', 'about', 'anywhere', 'again', 'they', '’re', 'most', 'fifty', 'within', 'give', 'any', 'only', 'twenty', 'using', 'all', 'least', 'because', 'everywhere', 'sometime', 'twelve', 'anyhow', 'had', 'moreover', 'both', 'their', 'via', 'anyway', 'eleven', 'and', 're', 'thereafter', 'herself', 'off', 'next', 'although', 'three', 'into', 'quite', 'our', 'what', 'being', 'there', 'fifteen', 'ten', \"'ve\", 'whereupon', 'top', 'seeming', 'but', 'hereupon', 'seems', 'yours', 'though', 'sandisk', 'upon', 'doing', 'my', 'why', 'been', 'still', 'afterwards', 'formerly', \"'m\", 'first', 'eight', 'once', '‘re', 'could', 'keep', 'less', 'every', 'those', 'your'}\n"
     ]
    }
   ],
   "source": [
    "additional_stop_words = {'card', 'sandisk', 'phone'}\n",
    "nlp.Defaults.stop_words |= additional_stop_words\n",
    "\n",
    "remove_stop_words = {'no', 'not'}\n",
    "nlp.Defaults.stop_words -= remove_stop_words\n",
    "\n",
    "print(\"Stoppwörter:\")\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9330de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorverarbeitung des Textes:\n",
    "def preprocess_text(text):\n",
    "    #Textformat muss ein String sein:\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    #Sonderzeichen entfernen:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    #Umwandlung in Kleinbuchstaben: \n",
    "    doc = nlp(text.lower())\n",
    "    #Stoppwörter entfernen und Lemmatisierung:\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b5d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einlesen erfolgreich.\n"
     ]
    }
   ],
   "source": [
    "#CSV-Datei einlesen & ausführen der definierten Vorverarbeitung:\n",
    "csv_datei = 'amazon_reviews_I.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_datei, on_bad_lines='skip')\n",
    "    print(\"Einlesen erfolgreich.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Fehler: Datei unter '{csv_datei}' nicht gefunden.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Fehler beim Parsen der CSV-Datei. Bitte überprüfen Sie das Dateiformat.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein unerwarteter Fehler ist aufgetreten: {e}\")\n",
    "    \n",
    "if 'df' in locals():\n",
    "    #Auswahl der Spalte 'reviewText'\n",
    "    if 'reviewText' in df.columns:\n",
    "        #Überprüfen und Bereinigen der Daten:\n",
    "        df['reviewText'] = df['reviewText'].astype(str)\n",
    "        df['cleaned_reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "    else:\n",
    "        print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b48091fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der BoW-Methode mittels scikit-Learn:\n",
    "if 'cleaned_reviewText' in df.columns:\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(df['cleaned_reviewText'])\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8a6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der BoW-Methode mittels numpy:\n",
    "def bow_with_numpy(texts):\n",
    "    #Erstellen des Vokabulars\n",
    "    vocab = list(set(\" \".join(texts).split()))\n",
    "    vocab.sort()\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    #Erstellen der BoW-Matrix\n",
    "    bow_matrix = np.zeros((len(texts), len(vocab)))\n",
    "    for i, text in enumerate(texts):\n",
    "        word_count = Counter(text.split())\n",
    "        for word, count in word_count.items():\n",
    "            if word in vocab_dict:\n",
    "                bow_matrix[i, vocab_dict[word]] = count\n",
    "    return bow_matrix, vocab\n",
    "\n",
    "if 'cleaned_reviewText' in df.columns:\n",
    "    bow_matrix_np, vocab_np = bow_with_numpy(df['cleaned_reviewText'])\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52e4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-Vektoren (scikit-learn):\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "BoW-Vektoren (numpy):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Anzeigen der beiden erstellten Vektoren:\n",
    "#scikit-Learn:\n",
    "if 'X_bow' in locals():\n",
    "    print(\"BoW-Vektoren (scikit-learn):\")\n",
    "    print(X_bow.toarray())\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#numpy:\n",
    "if 'bow_matrix_np' in locals():\n",
    "    print(\"BoW-Vektoren (numpy):\")\n",
    "    print(bow_matrix_np)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d87e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementierung der Tf-idf-Methode:\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3372010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktion zum Anzeigen der Themen:\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}:\")\n",
    "        topic_words = [words[i] for i in topic.argsort()[:-top_n - 1:-1] if i < len(words)]\n",
    "        print(\" \".join(topic_words)) \n",
    "\n",
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 3\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9447305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.7297212921835815\n",
      "LDA Coherence-Score: 0.5727879294566075\n"
     ]
    }
   ],
   "source": [
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "467a3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.7175079177344561\n",
      "LDA Coherence-Score: 0.5098698133379772\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 4\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8333b2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.6789818732117567\n",
      "LDA Coherence-Score: 0.5725315427436661\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 5\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cb50d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.7054908253615059\n",
      "LDA Coherence-Score: 0.6779561882538624\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 6\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbac2b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.7220728892583133\n",
      "LDA Coherence-Score: 0.6161614261493206\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 7\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "688e8966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.71978819600301\n",
      "LDA Coherence-Score: 0.4976688488594736\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 8\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18760842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.7119185436527763\n",
      "LDA Coherence-Score: 0.5247521596564902\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 9\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa43d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.6666381583967793\n",
      "LDA Coherence-Score: 0.5608451153894137\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 10\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff75903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.6780945350992795\n",
      "LDA Coherence-Score: 0.514846427000723\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 11\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fbd55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.6410692530401212\n",
      "LDA Coherence-Score: 0.45025296141795645\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 12\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65d3819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.6087004739276651\n",
      "LDA Coherence-Score: 0.4217297128186494\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 13\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4264ad52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.621554438480831\n",
      "LDA Coherence-Score: 0.5312768740209545\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 14\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9932b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.6149188633005382\n",
      "LDA Coherence-Score: 0.43836645263402835\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 15\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "718bad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.5684022502488646\n",
      "LDA Coherence-Score: 0.5081699573959502\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 16\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc62664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.6072140441162283\n",
      "LDA Coherence-Score: 0.43860700914416073\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 17\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c45c7f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.5651366734664416\n",
      "LDA Coherence-Score: 0.40078863012475013\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 18\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab19ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.5549158277418408\n",
      "LDA Coherence-Score: 0.39990911823655356\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 19\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1af289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Coherence-Score: 0.5547395176102348\n",
      "LDA Coherence-Score: 0.43644073872704603\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 20\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "else:\n",
    "    print(\"Fehler\")\n",
    "\n",
    "#Berechnung des Coherence-Scores für LSA: \n",
    "lsa_topic_words = []\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    lsa_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary([text.split() for text in df['cleaned_reviewText']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['cleaned_reviewText']]\n",
    "\n",
    "lsa_coherence_model = CoherenceModel(topics=lsa_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lsa_coherence_score = lsa_coherence_model.get_coherence()\n",
    "\n",
    "#Berechnung des Coherence-Scores für LDA: \n",
    "lda_topic_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    lda_topic_words.append([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "lda_coherence_model = CoherenceModel(topics=lda_topic_words, texts=[text.split() for text in df['cleaned_reviewText']], dictionary=dictionary, coherence='c_v')\n",
    "lda_coherence_score = lda_coherence_model.get_coherence()\n",
    "\n",
    "#Ausgabe der berechneten Scores für LSA & LDA:\n",
    "print(f\"LSA Coherence-Score: {lsa_coherence_score}\")\n",
    "print(f\"LDA Coherence-Score: {lda_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34c2419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topics:\n",
      "Topic 0:\n",
      "nan tablet price sd good micro stuff time speed lag\n",
      "Topic 1:\n",
      "work great buy sd samsung memory no galaxy use product\n",
      "Topic 2:\n",
      "sd micro product card recommend transfer samsung galaxy adapter great\n",
      "Topic 3:\n",
      "good no gb price fast speed ve month sd issue\n",
      "Topic 4:\n",
      "no issue problem video month ve camera transfer hd record\n",
      "Topic 5:\n",
      "tablet samsung memory tab gb expand no purchase galaxy happy\n",
      "LDA Topics:\n",
      "Topic 0:\n",
      "ve work card problem buy no gopro tablet hero month\n",
      "Topic 1:\n",
      "price no great work issue memory good sd tablet storage\n",
      "Topic 2:\n",
      "nan speak feature car key entire rt microsdxc hold drive\n",
      "Topic 3:\n",
      "work samsung galaxy great speed use not format buy space\n",
      "Topic 4:\n",
      "great work product sd camera micro want adapter happy recommend\n",
      "Topic 5:\n",
      "work fast buy memory video far smartphone good stuff sd\n"
     ]
    }
   ],
   "source": [
    "#Häufigste Themen mittels LSA:\n",
    "n_topics = 6\n",
    "\n",
    "if 'tfidf_matrix' in locals():\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    print(\"LSA Topics:\")\n",
    "    print_topics(lsa_model, vectorizer)\n",
    "    \n",
    "#Häufigste Themen mittels LDA:\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_topic_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    print(\"LDA Topics:\")\n",
    "    print_topics(lda_model, vectorizer)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60b0157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumentenzuweisungen für LSA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>great quality product purchase gopro hero   wo...</td>\n",
       "      <td>micro sd work great describe beware micro sd a...</td>\n",
       "      <td>good price</td>\n",
       "      <td>no issue</td>\n",
       "      <td>purchase expand memory tablet   work tablet ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>purchase use samsung camera work great picture...</td>\n",
       "      <td>great quality product purchase gopro hero   wo...</td>\n",
       "      <td>good value money   class rate fast speed good ...</td>\n",
       "      <td>buy surface pro ve month ve no problem fast st...</td>\n",
       "      <td>instal samsung tablet tab    definitely perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>work great galaxy s not need format pop work g...</td>\n",
       "      <td>ultra   gb micro sd xc   perfect card</td>\n",
       "      <td>ultra gb memory show no sign exactly need   go...</td>\n",
       "      <td>item ve month record hd video gopro hero problem</td>\n",
       "      <td>purchase use samsung tablet work perfectly sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nan</td>\n",
       "      <td>buy new samsung galaxy tab   work great allow ...</td>\n",
       "      <td>read write speed well samsung sd previously he...</td>\n",
       "      <td>good product get tired take picture memory goo...</td>\n",
       "      <td>store record video information dash camera   n...</td>\n",
       "      <td>expand samsung tab   tablet gb gb work fast no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>purchase use samsung tablet work perfectly sto...</td>\n",
       "      <td>gb micro sd class   reliable sd amazing transf...</td>\n",
       "      <td>class   speed way fly gb usable space think good</td>\n",
       "      <td>month galaxy s no issue plenty fast transfer c...</td>\n",
       "      <td>second   gb microsdhc card buy   instal samsun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic 0                                            Topic 1  \\\n",
       "0     nan  great quality product purchase gopro hero   wo...   \n",
       "1     nan  purchase use samsung camera work great picture...   \n",
       "2     nan  work great galaxy s not need format pop work g...   \n",
       "3     nan  buy new samsung galaxy tab   work great allow ...   \n",
       "4     nan  purchase use samsung tablet work perfectly sto...   \n",
       "\n",
       "                                             Topic 2  \\\n",
       "0  micro sd work great describe beware micro sd a...   \n",
       "1  great quality product purchase gopro hero   wo...   \n",
       "2              ultra   gb micro sd xc   perfect card   \n",
       "3  read write speed well samsung sd previously he...   \n",
       "4  gb micro sd class   reliable sd amazing transf...   \n",
       "\n",
       "                                             Topic 3  \\\n",
       "0                                         good price   \n",
       "1  good value money   class rate fast speed good ...   \n",
       "2  ultra gb memory show no sign exactly need   go...   \n",
       "3  good product get tired take picture memory goo...   \n",
       "4   class   speed way fly gb usable space think good   \n",
       "\n",
       "                                             Topic 4  \\\n",
       "0                                           no issue   \n",
       "1  buy surface pro ve month ve no problem fast st...   \n",
       "2   item ve month record hd video gopro hero problem   \n",
       "3  store record video information dash camera   n...   \n",
       "4  month galaxy s no issue plenty fast transfer c...   \n",
       "\n",
       "                                             Topic 5  \n",
       "0  purchase expand memory tablet   work tablet ca...  \n",
       "1  instal samsung tablet tab    definitely perfor...  \n",
       "2  purchase use samsung tablet work perfectly sto...  \n",
       "3  expand samsung tab   tablet gb gb work fast no...  \n",
       "4  second   gb microsdhc card buy   instal samsun...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumentenzuweisungen für LDA:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>purchase version expand memory gb android tabl...</td>\n",
       "      <td>review counting   need justification buy loo...</td>\n",
       "      <td>nan</td>\n",
       "      <td>work expect   high transfer speed   nice extra...</td>\n",
       "      <td>month work finei sd encrypt little bit long tr...</td>\n",
       "      <td>come nice little adapter microsdhc use normal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not micro sd work not buy install work fine fi...</td>\n",
       "      <td>fast microsd compare class   speed look nice w...</td>\n",
       "      <td>nan</td>\n",
       "      <td>initially insert samsung galaxy s recognize no...</td>\n",
       "      <td>year go strong boost memory tablet store movie...</td>\n",
       "      <td>read write speed well samsung sd previously he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>job fairly cheap worth return exchange defecti...</td>\n",
       "      <td>skip beat file transfer speedy corruption issu...</td>\n",
       "      <td>nan</td>\n",
       "      <td>second   gb microsdhc card buy   instal samsun...</td>\n",
       "      <td>company trust offer good product low pricecome...</td>\n",
       "      <td>standard micro sd recommend purchase dad expan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>expand samsung tab   tablet gb gb work fast no...</td>\n",
       "      <td>product supply describe gb micro sdhc class   ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>load home movie samsung galaxy tab   copy proc...</td>\n",
       "      <td>work fine instal new ms surface protoo bad win...</td>\n",
       "      <td>purchase new nokia lumia   smart recently purc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ve order sd card good overall shipping excelle...</td>\n",
       "      <td>get   gb fill keep get insufficient disk space...</td>\n",
       "      <td>nan</td>\n",
       "      <td>amazed speed sd smart hd video no long jump bu...</td>\n",
       "      <td>arrive quickly perfect condition   work camera...</td>\n",
       "      <td>work expect spring high capacity   think bit c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Topic 0  \\\n",
       "0  purchase version expand memory gb android tabl...   \n",
       "1  not micro sd work not buy install work fine fi...   \n",
       "2  job fairly cheap worth return exchange defecti...   \n",
       "3  expand samsung tab   tablet gb gb work fast no...   \n",
       "4  ve order sd card good overall shipping excelle...   \n",
       "\n",
       "                                             Topic 1 Topic 2  \\\n",
       "0    review counting   need justification buy loo...     nan   \n",
       "1  fast microsd compare class   speed look nice w...     nan   \n",
       "2  skip beat file transfer speedy corruption issu...     nan   \n",
       "3  product supply describe gb micro sdhc class   ...     nan   \n",
       "4  get   gb fill keep get insufficient disk space...     nan   \n",
       "\n",
       "                                             Topic 3  \\\n",
       "0  work expect   high transfer speed   nice extra...   \n",
       "1  initially insert samsung galaxy s recognize no...   \n",
       "2  second   gb microsdhc card buy   instal samsun...   \n",
       "3  load home movie samsung galaxy tab   copy proc...   \n",
       "4  amazed speed sd smart hd video no long jump bu...   \n",
       "\n",
       "                                             Topic 4  \\\n",
       "0  month work finei sd encrypt little bit long tr...   \n",
       "1  year go strong boost memory tablet store movie...   \n",
       "2  company trust offer good product low pricecome...   \n",
       "3  work fine instal new ms surface protoo bad win...   \n",
       "4  arrive quickly perfect condition   work camera...   \n",
       "\n",
       "                                             Topic 5  \n",
       "0  come nice little adapter microsdhc use normal ...  \n",
       "1  read write speed well samsung sd previously he...  \n",
       "2  standard micro sd recommend purchase dad expan...  \n",
       "3  purchase new nokia lumia   smart recently purc...  \n",
       "4  work expect spring high capacity   think bit c...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 1: nan, tablet, price, sd, good, micro, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 2: work, great, buy, sd, samsung, memory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic 3: sd, micro, product, card, recommend, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic 4: good, no, gb, price, fast, speed, ve,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic 5: no, issue, problem, video, month, ve,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Topic 6: tablet, samsung, memory, tab, gb, exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Top Words\n",
       "0  Topic 1: nan, tablet, price, sd, good, micro, ...\n",
       "1  Topic 2: work, great, buy, sd, samsung, memory...\n",
       "2  Topic 3: sd, micro, product, card, recommend, ...\n",
       "3  Topic 4: good, no, gb, price, fast, speed, ve,...\n",
       "4  Topic 5: no, issue, problem, video, month, ve,...\n",
       "5  Topic 6: tablet, samsung, memory, tab, gb, exp..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic 1: ve, work, card, problem, buy, no, gop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic 2: price, no, great, work, issue, memory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic 3: nan, speak, feature, car, key, entire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic 4: work, samsung, galaxy, great, speed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic 5: great, work, product, sd, camera, mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Topic 6: work, fast, buy, memory, video, far, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Top Words\n",
       "0  Topic 1: ve, work, card, problem, buy, no, gop...\n",
       "1  Topic 2: price, no, great, work, issue, memory...\n",
       "2  Topic 3: nan, speak, feature, car, key, entire...\n",
       "3  Topic 4: work, samsung, galaxy, great, speed, ...\n",
       "4  Topic 5: great, work, product, sd, camera, mic...\n",
       "5  Topic 6: work, fast, buy, memory, video, far, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Funktion zum Zuweisen der Themen zu den Dokumenten:\n",
    "def assign_topics(topic_matrix, n_top_documents=5):\n",
    "    assignments = {}\n",
    "    for topic_idx, topic in enumerate(topic_matrix.T):\n",
    "        assignments[f'Topic {topic_idx}'] = []\n",
    "        top_document_indices = topic.argsort()[:-n_top_documents - 1:-1]\n",
    "        for doc_index in top_document_indices:\n",
    "            assignments[f'Topic {topic_idx}'].append(df['cleaned_reviewText'].iloc[doc_index])\n",
    "    return assignments\n",
    "\n",
    "#Funktion zum Erstellen eines DataFrames:\n",
    "def create_topic_dataframe(model, vectorizer, n_top_words=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1] if i < len(words)]\n",
    "        topics.append(f\"Topic {topic_idx + 1}: \" + \", \".join(topic_words))\n",
    "    return pd.DataFrame(topics, columns=[\"Top Words\"])\n",
    "\n",
    "if 'lsa_topic_matrix' in locals() and 'lda_topic_matrix' in locals():\n",
    "    #Dokumentzuweisungen für LSA:\n",
    "    print(\"Dokumentenzuweisungen für LSA:\")\n",
    "    lsa_assignments = assign_topics(lsa_topic_matrix)\n",
    "    lsa_assignments_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in lsa_assignments.items()]))\n",
    "    display(lsa_assignments_df)\n",
    "\n",
    "    #Dokumentzuweisungen für LDA:\n",
    "    print(\"Dokumentenzuweisungen für LDA:\")\n",
    "    lda_assignments = assign_topics(lda_topic_matrix)\n",
    "    lda_assignments_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in lda_assignments.items()]))\n",
    "    display(lda_assignments_df)\n",
    "\n",
    "    #DataFrames für die Themen:\n",
    "    lsa_topics_df = create_topic_dataframe(lsa_model, vectorizer)\n",
    "    lda_topics_df = create_topic_dataframe(lda_model, vectorizer)\n",
    "\n",
    "    print(\"LSA Topics:\")\n",
    "    display(lsa_topics_df)\n",
    "\n",
    "    print(\"LDA Topics:\")\n",
    "    display(lda_topics_df)\n",
    "else:\n",
    "    print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec44c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
